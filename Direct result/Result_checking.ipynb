{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Result checking.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "42JdXPyHlCW7",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 122
        },
        "outputId": "a9d2cfbf-5ba5-4ae5-b7f6-9a854b460c84"
      },
      "source": [
        "import PyPDF2\n",
        "import nltk\n",
        "nltk.download('stopwords')\n",
        "import operator\n",
        "from nltk.corpus import stopwords\n",
        "stop_words = stopwords.words('english')\n",
        "import string\n",
        "nltk.download('punkt')\n",
        "from nltk.tokenize import word_tokenize\n",
        "import re\n",
        "import os\n",
        "nltk.download('punkt')\n",
        "import collections\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n",
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n",
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xOPWIOsDl9bu",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 213
        },
        "outputId": "4920e049-3c6d-4269-896c-8ec220f512ab"
      },
      "source": [
        "pip install PyPDF2"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting PyPDF2\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/b4/01/68fcc0d43daf4c6bdbc6b33cc3f77bda531c86b174cac56ef0ffdb96faab/PyPDF2-1.26.0.tar.gz (77kB)\n",
            "\r\u001b[K     |████▎                           | 10kB 18.6MB/s eta 0:00:01\r\u001b[K     |████████▌                       | 20kB 1.7MB/s eta 0:00:01\r\u001b[K     |████████████▊                   | 30kB 2.2MB/s eta 0:00:01\r\u001b[K     |█████████████████               | 40kB 2.5MB/s eta 0:00:01\r\u001b[K     |█████████████████████▏          | 51kB 2.0MB/s eta 0:00:01\r\u001b[K     |█████████████████████████▍      | 61kB 2.2MB/s eta 0:00:01\r\u001b[K     |█████████████████████████████▋  | 71kB 2.5MB/s eta 0:00:01\r\u001b[K     |████████████████████████████████| 81kB 2.3MB/s \n",
            "\u001b[?25hBuilding wheels for collected packages: PyPDF2\n",
            "  Building wheel for PyPDF2 (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for PyPDF2: filename=PyPDF2-1.26.0-cp36-none-any.whl size=61086 sha256=38b9d446676e2a2bb1d24032d1e519651c8bf8ca17eb9b26ca6e24a77802d526\n",
            "  Stored in directory: /root/.cache/pip/wheels/53/84/19/35bc977c8bf5f0c23a8a011aa958acd4da4bbd7a229315c1b7\n",
            "Successfully built PyPDF2\n",
            "Installing collected packages: PyPDF2\n",
            "Successfully installed PyPDF2-1.26.0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0TS5bbidmWGP",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import pandas as pd"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZDdiRnCOl09P",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import numpy as np\n",
        "df1 = pd.read_csv(\"/content/document_embeddings.csv\")\n",
        "document_embeddings1 = df1.to_dict('list')\n",
        "word_embedding1 = np.load('/content/word_embedding.npy')\n",
        "worddic = np.load('/content/worddic_10000.npy',allow_pickle=True)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0NvPAUAvoeuw",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "dictionary = np.load('/content/dictionary.npy',allow_pickle=True)\n",
        "dictionary=dictionary.tolist()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "x_eM-424mpY-",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "alpha = [\"a\",\"b\",\"c\",\"d\",\"e\",\"f\",\"g\",\"h\",\"j\",\"k\",\"l\",\"m\",\"n\",\"o\",\"p\",\"q\",\"r\",\"s\",\"u\",\"v\",\"w\",\"x\",\"y\",\"z\"]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FxJfRO5elNCU",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def get_cosine_similarity(feature_vec_1, feature_vec_2):    \n",
        "    return cosine_similarity(feature_vec_1.reshape(1, -1), feature_vec_2.reshape(1, -1))[0][0]\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DAFEgeDn6cAv",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "query = \"gradient clipping\"\n",
        "quer_list = []\n",
        "def filter_query(q):\n",
        "  tokens = q.split()\n",
        "  tokens = [w.lower() for w in tokens]\n",
        "  table = str.maketrans('', '', string.punctuation)\n",
        "  stripped = [w.translate(table) for w in tokens]\n",
        "  words = [word for word in stripped if word.isalpha()]\n",
        "  words = [w for w in words if not w in alpha]\n",
        "  stop_words = set(stopwords.words('english'))\n",
        "  words = [w for w in words if not w in stop_words]\n",
        "  return words"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0MowT3qFLNbJ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "query_list= filter_query(query)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Hjb7toqpoCQw",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "worddic1=worddic.tolist()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CsOd9Zg3LazP",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "related_docs=[]\n",
        "\n",
        "for f in query_list:\n",
        "  a = worddic1[f]\n",
        "  for x in a:\n",
        "    related_docs.append(x[0])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uiDi2l0tQFjn",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "query_embedding =[]\n",
        "for f in query_list:\n",
        "  query_embedding.append(word_embedding1[dictionary[f]])\n",
        "\n",
        "query_embedding_mean = np.mean(np.array(query_embedding),axis=0)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vNnPTHxoMtcU",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "related_docs_unique = list(set(related_docs))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "smDz0J-vRhdZ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def similar_func(related_docs_unique,query_embedding_mean):                               \n",
        "\n",
        "  similarity={}\n",
        "\n",
        "  for f in range(len(related_docs_unique)):\n",
        "    score= get_cosine_similarity(np.array(document_embeddings1[related_docs_unique[f]]),query_embedding_mean)\n",
        "    similarity[related_docs_unique[f]]=score\n",
        "\n",
        "  similarity_sort=sorted(similarity.items(), key=operator.itemgetter(1),reverse=True)\n",
        "  return(similarity_sort)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YCja-kcCZPc4",
        "colab_type": "code",
        "outputId": "419dc0c1-e960-4dbc-c00f-ca13567293e4",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 756
        }
      },
      "source": [
        "similar_func(related_docs_unique,query_embedding_mean)"
      ],
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[('Natural Language Processing with TensorFlow ( PDFDrive.com )-240.pdf',\n",
              "  0.6701666188975917),\n",
              " ('Natural Language Processing with TensorFlow ( PDFDrive.com )-263.pdf',\n",
              "  0.5835365686852821),\n",
              " ('Natural Language Processing with TensorFlow ( PDFDrive.com )-241.pdf',\n",
              "  0.5826147611661008),\n",
              " ('Natural Language Processing with TensorFlow ( PDFDrive.com )-239.pdf',\n",
              "  0.533671082949282),\n",
              " ('Natural Language Processing with TensorFlow ( PDFDrive.com )-289.pdf',\n",
              "  0.5116844578617191),\n",
              " ('Natural Language Processing with TensorFlow ( PDFDrive.com )-238.pdf',\n",
              "  0.49216157518568415),\n",
              " ('Natural Language Processing with TensorFlow ( PDFDrive.com )-225.pdf',\n",
              "  0.38376698251395064),\n",
              " ('Natural Language Processing with TensorFlow ( PDFDrive.com )-226.pdf',\n",
              "  0.3409050186490209),\n",
              " ('Natural Language Processing with TensorFlow ( PDFDrive.com )-44.pdf',\n",
              "  0.33073174874761824),\n",
              " ('Natural Language Processing with TensorFlow ( PDFDrive.com )-227.pdf',\n",
              "  0.31529426480861833),\n",
              " ('Natural Language Processing with TensorFlow ( PDFDrive.com )-252.pdf',\n",
              "  0.25295620769564253),\n",
              " ('Natural Language Processing with TensorFlow ( PDFDrive.com )-257.pdf',\n",
              "  0.25135054476202434),\n",
              " ('Natural Language Processing with TensorFlow ( PDFDrive.com )-80.pdf',\n",
              "  0.24116239036010048),\n",
              " ('Natural Language Processing with TensorFlow ( PDFDrive.com )-37.pdf',\n",
              "  0.24065159668551303),\n",
              " ('Natural Language Processing with TensorFlow ( PDFDrive.com )-221.pdf',\n",
              "  0.23645466317800645),\n",
              " ('Natural Language Processing with TensorFlow ( PDFDrive.com )-312.pdf',\n",
              "  0.18644413898995857),\n",
              " ('Natural Language Processing with TensorFlow ( PDFDrive.com )-254.pdf',\n",
              "  0.16702484156880987),\n",
              " ('Natural Language Processing with TensorFlow ( PDFDrive.com )-36.pdf',\n",
              "  0.13138110712272663),\n",
              " ('Natural Language Processing with TensorFlow ( PDFDrive.com )-141.pdf',\n",
              "  0.09630979667707609),\n",
              " ('Natural Language Processing with TensorFlow ( PDFDrive.com )-137.pdf',\n",
              "  -0.018787671096905906),\n",
              " ('Natural Language Processing with TensorFlow ( PDFDrive.com )-88.pdf',\n",
              "  -0.033955033441611035)]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 31
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6f0bG96llQzH",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IZm1OwqFlQ1R",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 213
        },
        "outputId": "97e6b634-a1bf-429c-ca9e-9caf81c35f53"
      },
      "source": [
        "pip install pdfminer.six\n"
      ],
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting pdfminer.six\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/0b/04/f62d5834c2bdf90afcaeb23bb5241033c44e27000de64ad8472253daa4a8/pdfminer.six-20200402-py3-none-any.whl (5.6MB)\n",
            "\u001b[K     |████████████████████████████████| 5.6MB 2.9MB/s \n",
            "\u001b[?25hRequirement already satisfied: chardet; python_version > \"3.0\" in /usr/local/lib/python3.6/dist-packages (from pdfminer.six) (3.0.4)\n",
            "Requirement already satisfied: sortedcontainers in /usr/local/lib/python3.6/dist-packages (from pdfminer.six) (2.1.0)\n",
            "Collecting pycryptodome\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/af/16/da16a22d47bac9bf9db39f3b9af74e8eeed8855c0df96be20b580ef92fff/pycryptodome-3.9.7-cp36-cp36m-manylinux1_x86_64.whl (13.7MB)\n",
            "\u001b[K     |████████████████████████████████| 13.7MB 261kB/s \n",
            "\u001b[?25hInstalling collected packages: pycryptodome, pdfminer.six\n",
            "Successfully installed pdfminer.six-20200402 pycryptodome-3.9.7\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jcmcDSm2lQ34",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from pdfminer.high_level import extract_text\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Xz6iO_vfBTUb",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 598
        },
        "outputId": "c5727c23-0703-47ec-da5f-c69fb88c08d5"
      },
      "source": [
        "text = extract_text(\"/content/Natural Language Processing with TensorFlow ( PDFDrive.com )-34.pdf\")\n",
        " \n",
        "print(text)"
      ],
      "execution_count": 34,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Next, we can have a sentence planner that corrects any linguistic mistakes  \n",
            "(for example, morphological or grammar), which we might have in the phrases.  \n",
            "For examples, a sentence planner outputs the phrase, I go house as I go home; it can  \n",
            "use a database of rules, which contains the correct way of conveying meanings  \n",
            "(for example, the need of a preposition between a verb and the word house).\n",
            "\n",
            "Now we can generate a set of phrases for a given set of statistics using a HMM. \n",
            "Then, we need to aggregate these phrases in such a way that an essay made from the \n",
            "collection of phrases is human readable and flows correctly. For example, consider \n",
            "the three phrases, Player 10 of the Barcelona team scored a goal in the second half, Barcelona \n",
            "played against Manchester United, and Player 3 from Manchester United got a yellow card \n",
            "in the first half; having these sentences in this order does not make much sense. We \n",
            "like to have them in this order: Barcelona played against Manchester United, Player 3 \n",
            "from Manchester United got a yellow card in the first half, and Player 10 of the Barcelona \n",
            "team scored a goal in the second half. To do this, we use a discourse planner; discourse \n",
            "planners can order and structure a set of messages that need to be conveyed.\n",
            "\n",
            "Now we can get a set of arbitrary test statistics and obtain an essay explaining the \n",
            "statistics by following the preceding workflow, which is depicted in Figure 1.3:\n",
            "\n",
            "Figure 1.3: A step from a classical approach example of solving a language modelling task\n",
            "\n",
            "Here, it is important to note that this is a very high level explanation that only covers the \n",
            "main general-purpose components that are most likely to be included in the traditional \n",
            "way of NLP. The details can largely vary according to the particular application we are \n",
            "interested in solving. For example, additional application-specific crucial components \n",
            "might be needed for certain tasks (a rule base and an alignment model in machine \n",
            "translation). However, in this book, we do not stress about such details as the main \n",
            "objective here is to discuss more modern ways of natural language processing.\n",
            "\n",
            "[ 9 ]\n",
            "\n",
            "Chapter 1\f\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0OFPwXMwBTSM",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nAoLzGe8BTQB",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "L3J0jUbrBTM6",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FLOhQgXABTJR",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cV3GKs6JBTGZ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LUQu_TwvBTEI",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IzH3ZGaRBTAD",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}